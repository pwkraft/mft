## Working Paper:
# Moral Foundations of Political Reasoning - Investigating the Moral Underpinnings of Political Judgment

## Overview
This paper examines whether and under which conditions conservatives and liberals rely on different moral foundations when evaluating political parties and candidates. The analyses are based on the open-ended survey responses in the 2012 ANES.

## Abstract
According to Moral Foundations Theory, moral thinking is fundamentally structured by ﬁve innate intuitions (Haidt/Joseph 2008). Subsequent studies showed that liberals and conservatives systematically differ in their emphasis on these moral foundations (e.g. Graham et al. 2009). The goal of the paper proposed here is to investigate whether individuals also rely on different moral foundations when evaluating political candidates and parties. Based on open-ended survey responses in the 2012 American National Election Study, it will be examined to what extent the apparent diﬀerences in moral judgments between liberals and conservatives actually shape and structure individual reasoning and evaluations in the political context. More speciﬁcally, I utilize the moral word lists used by Graham et al. (2009) to identify references to basic moral intuitions when individuals report on their attitudes towards political parties and candidates. Furthermore, it is examined whether the relationships connecting ideology and moral reasoning are moderated by political interest or expertise.

## Keywords
Moral Foundations, Ideology, Political Reasoning

## Modeling strategy (needs changes):
1. logit model: mentioning any moral foundation?
2. multiple logits: mentioning each of the moral foundations
3. conditional logit: response pattern determined by most prevalent moral foundation
4. non-exclusive multinomial choice model / poisson-multinomial

## Current TO DOs
- I should add analyses that show that I can predict voting behavior by using the open-ended responses!!!
- I should think about a middle ground between the merging of all responses for one individual and looking at every single response. For example, it could make sense to aggregate responses for candidate/party that is consistent/inconsistent or for example for like vs. dislike task. I could reduce the number of missings by aggregating the 8 items in 2 groups like that and still have some info about the item task
- use pre-election survey, since open-ended questions were asked in pre wave
- check response from stanley for review paper for arguments about causality: in how far is it possible to deduce moral reasoning in politics from open survey responses
- check whether standard deviation in plots is correct: se instead of sd! -> sqrt(sd^2/N)
- fill out application to get restricted ANES data (-> include Jason to use the data as well)
- think about ITT (eligibility) vs ATT (prior vote via IV) in discontinuity design
- change spell check mode at the end for final analysis

## General Ideas for Future Studies
- effects of campaign exposure
- closer look at differences b/w survey questions: pro/con, party/cand
- talk about possibility of structured topic models etc.
- effects of political discussions, political networks, network coherence etc.
- question: do partisans differ with regard to the degree to which they rely on moral foundations depending on in-party/out-party candidates?
- difference b/w positive and negative evaluations?
- other sources of heterogeneity: strength of partisanship/ideology?
- the anes 2012 also contains a big 5 battery, it would be nice to combine personality and moral foundations in a subsequent study.

## Limitations for conclusion
- uni-dimensional conceptualization of ideology (?)
- analyses based on dictinary -> what about structural topic models?
- didn't model survey responses in a single model (?)
- virtue and vice combined with in- and out-party evaluation!
- explanation why virtue/vice distinction is not necessary for now, thinking about a dimension in a positive or negative way is basically equivalent for moral foundations theory

## Ideas for MPSA presentation
- title: It's not only about what you say, but what you talk about (?)
- start with claim: analyzing how respondents talk about candidates predicts vote choice
	- example from opend -> who is voting for whom? (example includes information who the respondents mean)
	- that's almost a trivial example
	- now: another example where we remove any candidate/party names -> who is voting for whom now?
	-> introduction to mft!
- then: why is it beneficial to use open-ended responses?


## Review Jennifer:
First of all, I think you are off to a good start with this paper. That said, I do think the analyses can be broadened out in some of the ways you suggest at the end. But you are well on your way to having a solid paper that I could easily see being published (with some of the modifications I suggest below).

The motivation for analyzing open-ended (OE) responses first appears on the top of p. 3. I find your basic claim compelling but the point needs to be built up much more if you want the reader to be excited about what you are doing (and to view your efforts as a substantial contribution to the existing literature).  Right now, your set up is too understated. I recommend reading the attached forthcoming piece by Scott Clifford and colleagues--with their piece, you can make a stronger claim about weaknesses with existing approaches for showing the effect of the moral foundations (MFs) in everyday thinking about politics. I think what Scott is doing underscores your point (on p. 3 top) that existing approaches for showing the importance of MFs are limited, indirect, and not completely persuasive.

You invoke "political discourse" among citizens on p. 6 bottom but you don't actually have evidence that speaks to this point so be careful with your language. I also was a little unclear on the repeated references to the MFs beign used merely as a "rhetorical tool"  (e.g., by sophisticates only).  I think you have something in mind with this reference, but it is not entirely clear to the reader.

Going forward, I would encourage you to find other ways to demonstrate that the MFs are universal. You basically are trying to claim they are universal with a null result (lack of an interaction between the MFs and interest). I am guessing that some reviewers are not going to find this persuasive. Here you are going to have get creative:  think along the lines of response latency, implicit measures, and other "process"/task-based measures.  I don't have any specific recommendations here, but a reviewer might push you on your evidence for the MFs being "general" and "universal," necessitating a move away from observational data.

So a big detail that is missed is whether the likes/dislikes (LD) data were coded manually or by some other means.  This detail HAS to appear in the paper around p. 9. I also would like to know how many cases were deleted bc of the language issue mentioned on this page, and you need to be more clear in what you mean by "collapsing" the data on this page as well.

I am  worried about the subsample of people you are ultimately analyzing.  There is a huge missingness issue with the LD data (lots of people do not have any likes or dislikes ).  There also are missing observations when it comes to people self-reporting their ideological identification. So you already are analyzing a fairly sophisticated subset of the sample: those who report LDs and who have non-missing data on the ideology item. If I were a reviewer, I would be asking you how many cases you are losing because of missingness on either item.

The anomalous result related to Authority could be the result of liberals mentioning these words in their OE responses, but then rejecting them. A similar pattern was observed in Graham, Haidt, and Nosek 2009 (Study 4).  Scott and I explicitly coded for whether a foundation was being endorsed or rejected in our JOP piece--you might consider augmenting your coding to account for this. A moral word can be mentioned and either endorsed or rejected.

The dictionary in Graham, Haidt, and Nosek 2009 was not developed to code sermons explicitly. The authors wanted to analyze speeches and this ended up being what they analyzed after convention speeches proved unworkable.  You make several references to the dictionary being developed "for sermons" and that is not quite accurate. There also is a category of general moral words (not in the 2009 piece, but on the Your Morals website).  Scott and I found that these general moral words were used in the stem cell debate, so you might consider coding for these general words as well.

It is somewhat surprising that purity terms did not figure into the OE comments. You might consider checking the Your Morals website to make sure there is not a longer, more extensive list of Sanctity (new name for this foundation) words. Also, I would shy away from concluding that this foundation is not relevant in politics--there is a recent piece showing that it is one of the strongest correlates of culture war issues (see attached).

I think you need to clarify how the various subsets of the data were created  (all statements versus party evaluations vs presidential candidates). Are these distinct LD questions (e.g., one for candidates vs. parties)? It is not clear how the various figures came into being.

High interest respondents probably have more LDs overall, not just more moral words. You acknowledge this (on p. 16), but I think this has to be one of the first changes you make to the empirical analysis bc it is a real threat to the inferences you are drawing. As for future analyses, I appreciated the reference to the Roberts paper on topic modeling, and this might be a promising avenue to pursue.  I also thought the distinction between in- and out-party candidates (in terms of word usage) would be interesting to explore.

And finally, just FYI--footnote 2 is missing (or at least it was in the version I read).

Great paper--hope these comments help.  Look forward to seeing how this turns out!

## Review Stanley

1.  I suspect that the purity words are just not very likely to be used in an electoral context. It's not clear to me that adding other purity related words will help. One thing you might do is to see if they are used by people who are very religious. I'm not sure that will be very interesting though in the context of this paper.

2.  You may get sharper results if you move away from using just liberal-conservative self-identification. Using it is like trying to collapse multiple dimensions into one. And we know that the conservative label is especially heterogeneous (a significant proportion are libertarians and some even hold liberal political views). It wouldn't be too hard to use issue preferences to construct two dimensions of ideology -- economic and social. I've seen a couple of papers that find different patterns of relationship between the moral foundations measures and the two ideology dimensions. The Iyer et al. paper found that libertarians were low on all five of the foundations.

3. For the argument you're trying to make you really should have a better measure of sophistication than political interest. Information is the obvious candidate. So, for that reason alone, you might want to look at the likes/dislikes from another ANES survey. You probably want to replicate these results in any case.

4. Including a control for the total number of comments will be useful. There's a 1980 APSR paper by Eric Smith that argues that, holding all else constant, people who talk more are more likely to be scored in the higher categories of the levels of conceptualization. As you suggest, the more you say the more likely you are to say something using moral (or ideological) language.

*Question*: You suggested to look at a two-dimensional conceptualization of ideology. Do you think the framing of the paper should then focus on this aspect or should I still concentrate on the question of whether the moral foundation relationship is moderated by sophistication? Or would that be too much for a single paper?

*Response*: I think you need to do both. The first goal should be to show the relationship between the use of the moral foundation words and ideology. So, for this, you need to be careful about how you measure ideology. There are more than enough published works in political science that show why liberal-conservative identification is inadequate to use it for this purpose. Then you want to demonstrate that this is not just a function of sophistication -- that attentive people learn to pick up the "correct" language. For that you need a good measure of sophistication.

## Comments Graduate Student Colloquium (10/21/2014)
- the results are actually very different from Haidt (he originally argued that conservatives emphasize _all_ moral foundations, whereas the emphasis for liberals is lower for all but fairness and harm); the hypotheses in my paper are in fact not consistent with haidts original analysis (strictly speaking)
	- one reason for this could be that the emphasis of moral foundations is context-dependent, i.e. because of obama, all 5 mfs were important in 2012
	- therefore, it would make sense to look at previous years as well
- more information on the selection process of the cases is needed: individuals are more likely to answer the questions if they are already interested etc. -> more information on missing cases etc.
- better measure for sophistication is needed
- maybe the spanish responses could be translated using undergrad TAs (or google translate?)
- more information on which words were more prominent in each moral foundation. this could provide some insights as to why liberals score higher on authority dimension
- check out latent class models for word structure?